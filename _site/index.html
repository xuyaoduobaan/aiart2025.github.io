<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>AIART2024</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <!-- <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
  <link
    href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
    rel='stylesheet' type='text/css'>
  <link
    href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic'
    rel='stylesheet' type='text/css'>

  <!-- Plugin CSS -->
  <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/creative.min.css" rel="stylesheet">
  <link rel="icon" type="images/png" href="images/logo1.png">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top"><img src="images/logo.png" style="height: 50px;" /></a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
        data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">

        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#cfp">Call for Papers</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#dates">Dates</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#keynotes">Keynotes</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#program">Conference Program</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#ps">Technical Program Committee</a>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#ppl">Organizers</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#history">History</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#MIR">Partner</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#sponsorship">Sponsorship</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <header class="masthead text-center text-white d-flex">
    <div class="container my-auto">
      <div class="row">
        <div class="col-lg-10 mx-auto">
          <!-- <h2 class="text-uppercase"> -->
          <h2 style="font-size: 36px">
            <strong>The 6<sup>th</sup> IEEE Workshop on <br />
              <a href="https://2024.ieeeicme.org/" target="_blank" style="color: white">Artificial Intelligence for
                Art Creation</strong></a>
          </h2>
          <hr>
        </div>
        <div class="col-lg-8 mx-auto">
          <p class="text-faded mb-5" style="font-size: 24px; font-weight: bold;">Niagra Falls, Canada<br />July 15-19, 2024<br />
            Jointly with <a href="https://2024.ieeeicme.org/" target="_blank" style="color: white"><u>ICME
                2024</u></a>
            <br>
          </p>
        </div>
      </div>
    </div>
  </header>

  <section class="bg-primary" id="cfp">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Call for Papers</h2>
          <hr class="light my-4">
          <p class="text-faded mb-4" style="text-align: justify; color: white; font-size: 18px">
            Recent advances of AI-Generated Content (AIGC) have been an innovative engine for digital content generation. As an ever increasingly powerful tool, AI has gained great popularity across the whole spectrum of art, such as AI painting, composing, writing, virtual hosting, fashion, design, etc. Tools like Sora even demonstrates the ability to model and simulate the physical world. An era of AI-generated videos or movies is coming. Moreover, AI is also capable of understanding art, and evaluating the aesthetic value of art as well. AI has not only exhibited creativity to some extent, but also served as an enabling tool to discover the principles underneath creativity and imagination, which are traditional challenges for neuroscience, cognitive science, and psychology. Despite all these promising features of AI for Art, we still have to face the many challenges such as the explainability of generative models and the copyright issues of AI art works.
          </p>

          <p class="text-faded mb-4" style="text-align: justify; color: white; font-size: 18px">
            This is the 6<sup>th</sup> AIART workshop to be held in conjunction with ICME 2024 in Niagara Falls, Canada, and it aims to bring forward cutting-edge technologies and most recent advances in the area of AI art in terms of enabling creation, analysis, understanding, and rendering technologies.
          </p>

          <p class="text-faded mb-4" style="text-align: justify; color: white; font-size: 18px">
            The theme topic of AIART 2024 will be <strong>Big Models for Art Creation</strong>. We plan to invite 3 keynote speakers to present their insightful perspectives on AI art.
          </p>
          <p class="text-faded mb-4" style="text-align: justify; color: white; font-size: 18px">
            We sincerely invite high-quality papers presenting or addressing issues related to AI art, including but not limited to the following topics:
          </p>
          <ul style="color: white">
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                Affective computing for AI Art
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                Theory and practice of AI creativity
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                Neuroscience, cognitive science and psychology for AI Art
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
              Explainable AI (XAI) for art 
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI Art for metaverse
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for painting generation
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for 3D content generation
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for video and movie
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
              AI for cultural heritage
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for sound synthesis, music composition, performance, and instrument design
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for poem composing and synthesis
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for typography and graphic design
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for fashion, makeup, and virtual hosting
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for multimodal and cross-modal art generation
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for art style transfer
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                AI for aesthetics understanding, analysis, assessment and prediction
              </div>
            </li>
            <li>
              <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                Authentication and copyright issues of AI artworks
              </div>
            </li>
          </ul>
          <br>
          <br>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            The authors of selected high-quality papers will be invited to submit an extended version to the Machine Intelligence Research (MIR) journal published by Springer.
          </p>

          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            Additionally, one Best Paper Award will be given.
          </p>

          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            AIART 2024 is also launching a demo track for artists to showcase their creative artworks in the form of in-person or online gallery. The demo track will provide a great opportunity for people to experience interactive artworks and communicate creative ideas. The submission guideline for the demo track follows
            that of the main ICME conference: <a href="https://2024.ieeeicme.org/author-information-and-submission-instructions/"
              target="_blank">https://2024.ieeeicme.org/author-information-and-submission-instructions/</a>.
          </p>

          <br>
          <br>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 24px">
            Paper Submission
          </p>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            Authors should prepare their manuscript according to the Guide for Authors of ICME available at Author
            Information and Submission Instructions: <a href="https://2024.ieeeicme.org/author-information-and-submission-instructions/"
              target="_blank">https://2024.ieeeicme.org/author-information-and-submission-instructions/</a>
          </p>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            Submission address: <a href="https://cmt3.research.microsoft.com/ICMEW2024"
              target="_blank">https://cmt3.research.microsoft.com/ICMEW2024</a>
          </p>

          <!-- <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px" >
              Submission address: <a href="http://2023.ieeeicme.org/author-info.html"  target="_blank" >http://2023.ieeeicme.org/author-info.html</a>
            </p> -->

          <br>
          <a class="btn btn-light btn-xl js-scroll-trigger" href="https://cmt3.research.microsoft.com/ICMEW2024"
            target="_blank">Submit link</a>
        </div>
      </div>
    </div>
  </section>

  <section id="dates">
    <div class="container">
      <div class="row">
        <div class="col-lg-6 mx-auto text-center">
          <h2 class="section-heading">Important Dates</h2>
          <hr class="dark my-4">
          <table width="100%" cellpadding="10" align="center">
            <tr>
              <td>
                <div class="text-muted" style="text-align: left; font-weight: bold;">Submissions due</div>
              </td>
              <td>
                <div class="text-muted" style="text-align: left;">April 6, 2024</div>
              </td>
            </tr>
            <tr>
              <td>
                <div class="text-muted" style="text-align: left; font-weight: bold;">Workshop date</div>
              </td>
              <td>
                <div class="text-muted" style="text-align: left;">July 19, 2024</div>
              </td>
            </tr>


          </table>
          <!-- <a class="btn btn-light btn-xl js-scroll-trigger" href="#services">Get Started!</a> -->
        </div>
      </div>
    </div>
  </section>


  <!-- first -->
  <section class="bg-primary" id="keynotes">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Keynotes (1/2)</h2>
          <hr class="dark my-4">
  
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 26px;">
            Keynote 1
          </p>
          <br>
          <div style="float:right; clear:both; top:30px;">
            <img src="images/Lamberto_Coccioli.jpg" style="height: 250px; border-radius: 10px;" />
          </div>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:
          </p>
  
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Lamberto Coccioli
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Title:
          </p>
  
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Is Artistic Creativity Worth Saving? Facing the Existential Challenge Posed by Generative AI
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Time:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            8:05 – 8:30, July 19, 2024
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Abstract:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            The sudden explosion of generative Artificial Intelligence and its creative applications in all art forms is
            forcing us
            to question the very meaning and purpose of art, giving us a glimpse of a world where human expressions of
            artistic
            creativity, from music composition to film making, from painting to architecture, may be largely supplanted by
            AI. Is
            this an inherently undesirable outcome? From my point of view as a composer, musician and music technologist I
            will
            consider the impact of generative AI on music and artistic creation, and outline what is at stake not only in
            terms of
            immediate concerns, for example around IP protection, but also from a wider philosophical perspective. Are
            there
            boundaries we shouldn’t cross in co-creation scenarios with AI agents? Ultimately, is human artistic
            creativity as we
            know it really worth preserving?
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Biography:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Lamberto Coccioli read architecture in Rome and music composition in Milan with Azio Corghi. Of lasting
            influence were
            also a series of field recording trips to remote areas of Colombia. Working for five years with Luciano Berio
            and
            research centre Tempo Reale in Florence, Lamberto pioneered new works with live electronics and immersive
            technologies,
            including his ground-breaking opera Magma. Moving to the UK in 2000, he joined Royal Birmingham Conservatoire
            (RBC) as
            Head of Music Technology, directed the 6-year, €3.1m project Integra – Fusing Music and Technology and founded
            Integra
            Lab, the internationally-renowned music interaction design research centre. Lamberto is Professor of Music and
            Technology and Associate Principal at RBC, where he is responsible for international development and large
            strategic
            projects, including the new RBC building with its state-of-the-art performance venues and innovative digital
            infrastructure. Research interests span from computer-assisted composition to augmented performance to the
            philosophy
            and ethics of technology.
          </p>
  
        </div>
      </div>
    </div>
  </section>

  <!-- second -->
    <section class="bg-primary" id="keynotes">
      <div class="container">
        <div class="row">
          <div class="col-lg-10 mx-auto text-center">
            <h2 class="section-heading text-white">Keynotes (2/2)</h2>
            <hr class="dark my-4">
    
            <p class="text-white" style="text-align: left; font-weight: bold; font-size: 26px;">
              Keynote 2
            </p>
            <br>
            <div style="float:right; clear:both; top:30px;">
              <img src="images/Shuai_Yang.jpg" style="height: 200px; border-radius: 10px;" />
            </div>
            <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
              Speaker:
            </p>
    
            <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
              Shuai Yang
            </p>
            <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
              Title:
            </p>
    
            <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
              AIGC for Human Artistic Rendering
            </p>
            <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
              Time:
            </p>
            <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
              10:00 – 10:25, July 19, 2024
            </p>
            <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
              Abstract:
            </p>
            <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
              Artistic portraits are ubiquitous in our daily lives and the creative industry. Intelligent human artistic
              rendering
              aims to automatically design artistic portraits based on real human portraits. This keynote will introduce our
              proposed
              three models for human artistic rendering: DualStyleGAN, VToonify, and StyleGANEX. First, I will discuss
              DualStyleGAN
              for image stylization, achieving exemplar-based high-resolution and style-controllable artistic portrait
              design. Next, I
              will extend DualStyleGAN to the video domain with VToonify, enabling vivid high-resolution artistic portrait
              video
              generation. Finally, I will introduce StyleGANEX, which extends portrait stylization to more general portrait
              image and
              video editing tasks.
    
            </p>
            <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
              Biography:
            </p>
            <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
              Shuai Yang received the B.S. and Ph.D. degrees (Hons.) in computer science from Peking University, Beijing,
              China, in
              2015 and 2020, respectively. He is currently an assistant professor with the Wangxuan Institute of Computer
              Technology,
              Peking University. His current research interests include image stylization, image translation and image
              editing. He was
              a Research Assistant Professor with the S-Lab, Nanyang Technological University, Singapore, from Mar. 2023 to
              Feb. 2024.
              He was a postdoctoral research fellow at Nanyang Technological University, from Oct. 2020 to Feb. 2023. He was
              a
              Visiting Scholar with the Texas A&M University, from Sep. 2018 to Sep. 2019. He was a Visiting Student with
              the National
              Institute of Informatics, Japan, from Mar. 2017 to Aug. 2017. He received the IEEE ICME 2020 Best Paper Awards
              and IEEE
              MMSP 2015 Top10 percent Paper Awards. He has served as the area chair of BMVC 2023/24 and ACM MM 2024.
            </p>
    
          </div>
        </div>
      </div>
    </section>

  <!-- third -->
  <!-- <section class="bg-primary" id="keynotes">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Keynotes (3/5)</h2>
          <hr class="dark my-4">

          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 26px;">
            Keynote 3
          </p>
          <br>
          <div style="float:right; clear:both; top:30px;">
            <img src="images/kang_zhang.png" style="height: 200px; border-radius: 10px;" />
          </div>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:
          </p>

          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Kang Zhang
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Title:
          </p>

          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Creating a Massive Open Metaverse Course (MOMC)
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Time:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            13:30 – 14:00, July 14, 2023
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Abstract:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Much effort has been made in using virtual reality (VR) technology to support massive open online course
            (MOOC) environments. This talk briefly reviews the latest research in VR/AR/XR application in education, and
            argues how immersive virtual educational experiences could be gained. We then introduce the new concept of
            Massive Open Metaverse Course (MOMC), combining MOOC and Metaverse and utilizing the latest volumetric video
            technology. We offer our vision on dual campus online education for HKUST 2.0, with a real case study, i.e.,
            the President’s First Lecture, under development at the Guangzhou campus. This is the world’s first true
            MOMC environment, providing immersive and realistic virtual and augmented reality experiences to both
            teachers and learners.
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Biography:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Kang Zhang is Acting Head and Professor of Computational Media and Arts, Information Hub, Hong Kong
            University of Science and Technology (Guangzhou), Professor of Division of Emerging Interdisciplinary Areas,
            HKUST, and Professor Emeritus of Computer Science, The University of Texas at Dallas. He was a Fulbright
            Distinguished Chair and an ACM Distinguished Speaker, and held academic positions in China, the UK,
            Australia and USA. Zhang's current research interests include computational aesthetics, visual languages,
            and generative art and design; and has published 8 books, and over 120 journal papers in these areas. He has
            delivered keynotes at art and design, computer science, and management conferences, and is on the editorial
            boards of Journal of Big Data, The Visual Computer, Journal of Visual Language and Computing, International
            Journal of Software Engineering and Knowledge Engineering, International Journal of Advanced Intelligence,
            and Visual Computing for Industry, Biomedicine, and Art.

          </p>

        </div>
      </div>
    </div>
  </section> -->

  <!-- fourth -->
  <!-- <section id="keynotes">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading">Keynotes (4/5)</h2>
          <hr class="dark my-4">

          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 26px;">
            Keynote 4
          </p>
          <br>
          <div style="float:right; clear:both; top:30px;">
            <img src="images/Bahareh_Nakisa.jpg" style="height: 250px; border-radius: 10px;" />
          </div>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:
          </p>

          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
            Bahareh Nakisa
          </p>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Title:
          </p>

          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
            Towards the wellbeing in space: measuring and monitoring the emotions of users immersed in meaningful
            virtual reality experiences
          </p>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Time:
          </p>
          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
            15:30 – 16:00, July 14, 2023
          </p>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Abstract:
          </p>
          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
            As the world advances in space exploration with current and planned stations in Low Earth Orbit (LEO), and
            long-duration missions to the Moon and Mars, a more diverse range of people will live and work in space.
            This increasing diversity in space exploration necessitates a deeper understanding of cultural context and
            diverse backgrounds when designing wellbeing solutions for astronauts. NASA identifies Five Hazards of Human
            Spaceflight: Radiation, Isolation and confinement, Distance from Earth, Gravity (or lack thereof), and
            Hostile/closed environments. Astronauts report challenges such as loneliness, boredom, disconnection,
            sensory deprivation, diminished cognitive performance, and stress while in orbit. In this talk we explore
            these challenges and how integrating extended reality (XR) environments, wearable sensors, and Artificial
            Intelligence can revolutionize astronauts' daily routines. We discuss how to mitigate environmental
            challenges and psychological stressors while promoting physical activity, motivation, and overall well-being
            in confined and isolated environment using advanced technologies.
          </p>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Biography:
          </p>
          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
            Dr. Bahareh Nakisa is a Lecturer of Applied AI and the course director of Applied AI at School of
            Information Technology, Deakin University, She received a B.Sc. degree in Soft Engineering from Iran in
            2008, Master of Computer Science from the National University of Malaysia in 2014, and a PhD in Computer
            Science (Artificial Intelligence) from the Queensland University of Technology (QUT), Australia in 2019. She
            started working in Industry as AI scientist and Lead AI scientist and then she joined School of Information
            Technology, Deakin University as a Lecturer of Applied AI in 2019. Her research interests are in the areas
            of artificial intelligence (AI), Deep learning, affective computing and time-series data analysis. She is
            particularly interested in the application of AI/DL models to solve real-world problems in applications such
            as healthcare, transportation, defence and space. She has published more than 39 publications in top-tier
            international venues in AI, computer science. She has secured more than AUD 2 million in external research
            and development funding.
          </p>
        </div>
      </div>
    </div>
  </section> -->

  <!-- fifth -->
  <!-- <section class="bg-primary" id="keynotes">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Keynotes (5/5)</h2>
          <hr class="dark my-4">

          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 26px;">
            Keynote 5
          </p>
          <br>
          <div style="float:right; clear:both; top:30px;">
            <img src="images/liu_dong.jpg" style="height: 200px; border-radius: 10px;" />
          </div>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:
          </p>

          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Dong Liu
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Title:
          </p>

          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Controllable Image Synthesis with Diffusion Models
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Time:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            16:45 – 17:15, July 14, 2023
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Abstract:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Diffusion models have demonstrated impressive capability in synthesizing photorealistic images given a few
            or even no words. These models may not fully satisfy user need, as normal users or artists intend to control
            the synthesized images with specific guidance, like overall layout, color, structure, object shape, and so
            on. We propose a method to adapt diffusion models for controllable image synthesis. Our method outperforms
            the existing methods and demonstrates multiple applications with its plausible generalization ability and
            flexible controllability.

          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Biography:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Dong Liu received the B.S. and Ph.D. degrees in electrical engineering from the University of Science and
            Technology of China (USTC), Hefei, China, in 2004 and 2009, respectively. He was a Member of Research Staff
            with Nokia Research Center, Beijing, China, from 2009 to 2012. He joined USTC as a faculty member in 2012
            and became a Professor in 2020. His research interests include image and video processing, coding, analysis,
            and data mining. He has authored or co-authored more than 200 papers in international journals and
            conferences, which were cited more than 12000 times according to Google Scholar (h-index is 42). He has more
            than 30 granted patents. He has several technique proposals adopted by standardization groups. He received
            2009 IEEE TCSVT Best Paper Award, VCIP 2016 Best 10% Paper Award, and ISCAS 2022 Grand Challenge Top
            Creativity Paper Award. He is a Senior Member of IEEE, CCF, and CSIG, an elected member of MSA-TC of IEEE
            CAS Society, and an elected member of Multimedia TC of CSIG. He serves or had served as the Chair of IEEE
            1857.11 Standard Working Subgroup (also known as Future Video Coding Study Group), a Guest Editor for IEEE
            TCSVT, an Organizing Committee Member for VCIP 2022, ChinaMM 2022, ICME 2021, etc.
          </p>

        </div>
      </div>
    </div>
  </section> -->


  <section id="program">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading">Conference Program</h2>
          <hr class="dark my-4">
          <img src="images/aiart2024_program.png" style="max-width: 100%;" />
        </div>
      </div>
    </div>
  </section>



  <section class="bg-primary" id="ps">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Technical Program Committee (Tentative)</h2>
          <hr class="light my-4">

          <h5 class="section-heading text-white" style="text-align: left; line-height: 1.8">
            <ul>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Ajay Kapur, California Institute of the Arts, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Alan Chamberlain, University of Nottingham, Nottingham
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Alexander Lerch, Georgia Institute of Technology, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Alexander Pantelyat, Johns Hopkins University, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Bahareh Nakisa, Deakin University, Australia
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Baoqiang Han, China Conservatory of Music, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Baoyang Chen, Central Academy of Fine Arts, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Beici Liang, Tencent Music Entertainment Group, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Bing Li, King Abdullah University of Science and Technology, Saudi Arabia
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Björn W. Schuller, Imperial College London, UK
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Bob Sturm, KTH Royal Institute of Technology, Sweden
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Carlos Castellanos, Rochester Institute of Technology, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Changsheng Xu, Institute of Automation, Chinese Academy of Sciences, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Dongmei Jiang, Northwestern Polytechnical University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Emma Young, BBC, UK
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Gus Xia, New York University Shanghai, China & Mohamed bin Zayed University of Artificial
                  Intelligence, United Arab Emirates
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Haifeng Li, Harbin Institute of Technology, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Haipeng Mi, Tsinghua University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Hongxun Yao, Harbin Institute of Technology, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Jesse Engel, Google, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Jia Jia, Tsinghua University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Jianyu Fan, Microsoft, Canada
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Jing Wang, Beijing Institute of Technology, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  John See, Multimedia University, Malaysia
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Juan Huang, Johns Hopkins University, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Junping Zhang, Fudan University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Kejun Zhang, Zhejiang University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Ke Lv, University of Chinese Academy of Sciences, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Kenneth Fields, Central Conservatory of Music, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Lai-Kuan Wong, Multimedia University, Malaysia
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Lamtharn Hanoi Hantrakul, ByteDance, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Lei Xie, Northwestern Polytechnical University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Lin Gan, Tianjin University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Long Ye, China University of Communication, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Maosong Sun, Tsinghua University, China </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Mei Han, Ping An Technology Art institute, USA </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Mengjie Qi, China Conservatory of Music, China </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Ming Zhang, Nanjing Art College, China </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Mohammad Naim Rastgoo, Queensland University of Technology, Australia </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Na Qi, Beijing University of Technology, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Nick Bryan-Kinns, Queen Mary University of London, UK
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Nina Kraus, Northwestern University, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Pengtao Xie, University of California, San Diego, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Philippe Pasquier, Simon Fraser University, Canada
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Qin Jin, Renmin University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Qiuqiang Kong, ByteDance, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Rebecca Fiebrink, University of London, UK
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Rick Taube, University of Illinois at Urbana-Champaign, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Roger Dannenberg, Carnegie Mellon University, USA
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Rongfeng Li, Beijing University of Posts and Telecommunications, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Rui Wang, Institute of Information Engineering, Chinese Academy of Sciences, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Ruihua Song, Renmin University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Shangfei Wang, University of Science and Technology of China, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Shasha Mao, Xidian University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Shiguang Shan, Institute of Computing Technology, Chinese Academy of Sciences, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Shiqi Wang, City University of Hong Kong, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Shun Kuremoto，Uchida Yoko Co.,Ltd，Japan
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Si Liu, Beihang University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Simon Lui, Huawei Technologies Co., Ltd, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Tiange Zhou, NetEase Cloud Music, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Weibei Dou, Tsinghua University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Weiming Dong, Institute of Automation, Chinese Academy of Sciences, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Wei-Ta Chu, National Chung Cheng University, Taiwan, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Wei Li, Fudan University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Weiwei Zhang, Dalian Maritime University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Wei Zhong, China University of Communication, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Wen-Huang Cheng, National Chiao Tung University, Taiwan, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Wenli Zhang, Beijing University of Technology, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Xi Shao, Nanjing University of Posts and Telecommunications, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Xiaojing Liang, NetEase Cloud Music, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Xiaopeng Hong, Harbin Institute of Technology, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Xiaoyan Sun, University of Science and Technology of China, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Xiaoying Zhang, China Rehabilitation Research Center, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Xihong Wu, Peking University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Xinfeng Zhang, University of Chinese Academy of Sciences, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Xu Tan, Microsoft Research Asia, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Yanchao Bi, Beijing Normal University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Yi Qin, Shanghai Conservatory of Music, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Ying-Qing Xu, Tsinghua University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Yirui Wu, Hohai University, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Yuanchun Xu, Xiaoice, China
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  Zhiyao Duan, University of Rochester, USA
                </div>
              </li>

          </h5>
        </div>
      </div>
    </div>
  </section>

  <section id="ppl">
    <div class="container">
      <div class="row">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading">Organizing Team</h2>
          <hr class="my-4">
        </div>
      </div>
    </div>
    <div class="container">
      <div class="row">
        <!-- Luntian Mou -->
        <div class="col-lg-6 col-md-8 text-center">
          <div class="service-box mt-5 mx-auto">
            <img class="mb-3" src="images/Luntian_Mou.jpeg" style="height: 200px; border-radius: 20px;" />

            <h5 class="mb-3"><a target="_blank" rel="noopener noreferrer" href="mailto:ltmou@bjut.edu.cn"
                style="color: black;">Luntian Mou</a></h5>

            <p class="text-muted mb-0" style="font-size: 0.9em;">Beijing University of Technology</p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Beijing, China</p>
            <a href="mailto:ltmou@bjut.edu.cn" target="_blank">
              <p class="text-muted mb-0" style="font-size: 0.9em;">ltmou@bjut.edu.cn</p>
            </a>
            <br>

            <p class="text-muted mb-0" style="font-size: 0.9em; " align="left">Dr. Luntian Mou is an Associate Professor with Beijing Institute of Artificial Intelligence (BIAI), School of Information Science and Technology, Beijing University of Technology. He was a Visiting Scholar with the University of California, Irvine, from 2019 to 2020. And he was a Postdoctoral Fellow at Peking University, from 2012 to 2014. He received Ph.D. in computer science from the University of Chinese Academy of Sciences in 2012. His current research interests include artificial intelligence, machine learning, pattern recognition, affective computing, multimedia computing, and brain-like computing. He has published in renowned journals such as TAFFC, TMM, TOMM, and ESWA. He holds 4 granted international patents (USA, Europe Union, Japan, and South Korea), and 3 granted China patents. He serves as a Co-Chair and Chief Editor of System subgroup in AVS workgroup, and the Chair of IEEE 1857.3 and IEEE 1857.7. He is an Expert of MPEG China Delegation. He is the Leading Chair for 3 published international standards (IEEE 1857.3-2023, IEEE 1857.7-2018, IEEE 1857.3-2013) and 3 China national standards (GB/T 33475.1-2019, GB/T 20090.11-2015, GB/T 20090.12-2015). He is the recipient of the Beijing Municipal Science and Technology Advancement Award, IEEE Outstanding Contribution to Standardization Award, and AVS Outstanding Contribution on 15th Anniversary Award. He serves as a Guest Editor for Machine Intelligence Research and a Reviewer for many important international journals and conferences such as TIP, TAFFC, TCSVT, TITS, and AAAI. He is a Senior Member of IEEE and CCF, and a Member of ACM. He is the Chair of the organizing committee of the 2023 CSIG Conference on Emotional Intelligence (CEI). He is the Founding Chair of the IEEE Workshop on Artificial Intelligence for Art Creation (AIART).</p>
          </div>
        </div>
        <!-- Feng Gao -->
        <div class="col-lg-6 col-md-8 text-center">
          <div class="service-box mt-5 mx-auto">
            <img class="mb-3" src="images/Feng_Gao.jpeg" style="height: 200px; border-radius: 20px;" />
            <h5 class="mb-3"><a target="_blank" rel="noopener noreferrer" href="mailto:gaofeng2018@tsinghua.edu.cn"
                style="color: black;">Feng Gao</a></h5>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Peking University</p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Beijing, China</p>
            <a href="mailto:gaofeng2018@tsinghua.edu.cn" target="_blank">
              <p class="text-muted mb-0" style="font-size: 0.9em;">gaof@pku.edu.cn</p>
            </a>
            <br>

            <p class="text-muted mb-0" style="font-size: 0.9em; " align="left">Dr. Feng Gao is an Assistant Professor with the School of Arts, Peking University. He has long researched in the disciplinary fields of AI and art, especially in AI painting. He co-initiated the international workshop of AIART. Currently, he is also enthusiastic in virtual human. He has demonstrated his AI painting system, called Daozi, in several workshops and drawn much attention. </p>
          </div>
        </div>
        <!-- Kejun Zhang -->
        <div class="col-lg-6 col-md-8 text-center">
          <div class="service-box mt-5 mx-auto">
            <img class="mb-3" src="images/Kejun_Zhang.jpg" style="height: 200px; border-radius: 20px;" />
            <h5 class="mb-3"><a target="_blank" rel="noopener noreferrer" href="https://person.zju.edu.cn/en/zhangkejun"
                style="color: black;">Kejun Zhang</a></h5>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Zhejiang University</p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Hangzhou, China</p>
            <a href="mailto:zhangkejun@zju.edu.cn" target="_blank">
              <p class="text-muted mb-0" style="font-size: 0.9em;">zhangkejun@zju.edu.cn</p>
            </a>
            <br>

            <p class="text-muted mb-0" style="font-size: 0.9em; " align="left">Dr. Kejun Zhang is a Professor with Zhejiang University, joint PhD supervisor on Design and Computer Science, Dean of Department of Industrial Design at College of Computer Science of Zhejiang University. He received his PhD degree from College of Computer Science and Technology, Zhejiang University in 2010. From 2008 to 2009, He was a visiting research scholar of University of Illinois at Urbana-Champaign, USA. In June 2013, he became a faculty of the College of Computer Science and Technology at Zhejiang University. His current research interests include Affective Computing，Design Science, Artificial Intelligence, Multimedia Computing and the understanding, modelling and innovation design of products and social management by computational means. He is now the PI of National Science Foundation of China, Co-PI of National Key Research and Development Program of China, and PIs of ten more other research programs. He has authored 4 books, more than 40 scientific papers.</p>
          </div>
        </div>
        <!-- Jiaying Liu -->
        <div class="col-lg-6 col-md-8 text-center">
          <div class="service-box mt-5 mx-auto">
            <a href="http://39.96.165.147/people/liujiaying.html">
              <img class="mb-3" src="images/Jiaying_Liu.jpeg" style="height: 200px; border-radius: 20px;" />
            </a>

            <h5 class="mb-3"><a target="_blank" rel="noopener noreferrer"
                href="http://39.96.165.147/people/liujiaying.html" style="color: black;">Jiaying Liu</a></h5>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Peking University</p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Beijing, China</p>
            <a href="mailto:liujiaying@pku.edu.cn" target="_blank">
              <p class="text-muted mb-0" style="font-size: 0.9em;">liujiaying@pku.edu.cn</p>
            </a>
            <br>

            <p class="text-muted mb-0" style="font-size: 0.9em; " align="left">Dr. Jiaying Liu is currently an Associate Professor with the Wangxuan Institute of Computer Technology, Peking University. She received the Ph.D. degree (Hons.) in computer science from Peking University, Beijing China, 2010. She has authored over 100 technical articles in refereed journals and proceedings, and holds 43 granted patents. Her current research interests include multimedia signal processing, compression, and computer vision. Dr. Liu is a Senior Member of IEEE, CSIG and CCF. She was a Visiting Scholar with the University of Southern California, Los Angeles, from 2007 to 2008. She was a Visiting Researcher with the Microsoft Research Asia in 2015 supported by the Star Track Young Faculties Award. She has served as a member of Membership Services Committee in IEEE Signal Processing Society, a member of Multimedia Systems & Applications Technical Committee (MSA TC), Visual Signal Processing and Communications Technical Committee (VSPC TC) in IEEE Circuits and Systems Society, a member of the Image, Video, and Multimedia (IVM) Technical Committee in APSIPA. She received the IEEE ICME 2020 Best Paper Awards and IEEE MMSP 2015 Top10% Paper Awards. She has also served as the Associate Editor of IEEE Trans. on Image Processing, and Elsevier JVCI, the Technical Program Chair of IEEE VCIP-2019/ACM ICMR-2021, the Publicity Chair of IEEE ICME-2020/ICIP-2019, and the Area Chair of CVPR-2021/ECCV-2020/ICCV-2019. She was the APSIPA Distinguished Lecturer (2016-2017).</p>
          </div>
        </div>
        <!-- Ling Fan -->
        <div class="col-lg-6 col-md-8 text-center">
          <div class="service-box mt-5 mx-auto">
            <img class="mb-3" src="images/Ling_Fan.jpg" style="height: 200px; border-radius: 20px;" />

            <h5 class="mb-3"><a target="_blank" rel="noopener noreferrer" href="mailto:lfan@tongji.edu.cn"
                style="color: black;">Ling Fan</a></h5>

            <p class="text-muted mb-0" style="font-size: 0.9em;">Tezign.com</p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Tongji University Design Artificial Intelligence Lab
            </p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Shanghai, China</p>
            <a href="mailto:lfan@tongji.edu.cn" target="_blank">
              <p class="text-muted mb-0" style="font-size: 0.9em;">lfan@tongji.edu.cn</p>
            </a>
            <br>

            <p class="text-muted mb-0" style="font-size: 0.9em; " align="left">Dr. Ling Fan is a Scholar and Entrepreneur to bridge machine intelligence with creativity. He is the founding chair and professor of Tongji University Design Artificial Intelligence Lab. Before, he held teaching position at the University of California at Berkeley and China Central Academy of Fine Arts. Dr. Fan co-founded Tezign.com, a leading technology start-up with the mission to build digital infrastructure for creative contents. Tezign is backed by top VCs like Sequoia Capital and Hearst Ventures. Dr. Fan is a World Economic Forum Young Global Leader, an Aspen Institute China Fellow, and Youth Committee member at the Future Forum. He is also a member of IEEE Global Council for Extended Intelligence. Dr. Fan received his doctoral degree from Harvard University and master's degree from Princeton University. He recently published From Universality of Computation to the Universality of Imagination, a book on how machine intelligence would influence human creativity.</p>
          </div>
        </div>
        <!-- Zeyu Wang -->
        <div class="col-lg-6 col-md-8 text-center">
          <div class="service-box mt-5 mx-auto">
            <a href="http://cislab.hkust-gz.edu.cn/members/zeyu-wang/">
              <img class="mb-3" src="images/zeyuwang.jpeg" style="height: 200px; border-radius: 20px;" />
            </a>

            <h5 class="mb-3"><a target="_blank" rel="noopener noreferrer"
                href="http://cislab.hkust-gz.edu.cn/members/zeyu-wang/" style="color: black;">Zeyu Wang</a></h5>

            <!-- <p class="text-muted mb-0" style="font-size: 0.9em;">Tezign.com</p> -->
            <p class="text-muted mb-0" style="font-size: 0.9em;">Hong Kong University of Science and <br> Technology
              (Guangzhou) </p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Guangzhou, China</p>
            <a href="mailto:zeyuwang@ust.hk" target="_blank">
              <p class="text-muted mb-0" style="font-size: 0.9em; ">zeyuwang@ust.hk</p>
            </a>
            <br>

            <p class="text-muted mb-0" style="font-size: 0.9em; " align="left">Dr. Zeyu Wang is an Assistant Professor of Computational Media and Arts (CMA) in the Information Hub at the Hong Kong University of Science and Technology (Guangzhou) and an Affiliate Assistant Professor in the Department of Computer Science and Engineering at the Hong Kong University of Science and Technology. He received a PhD from the Department of Computer Science at Yale University and a BS from the School of Artificial Intelligence at Peking University. He leads the Creative Intelligence and Synergy (CIS) Lab at HKUST(GZ) to study the intersection of Computer Graphics, Human-Computer Interaction, and Artificial Intelligence, with a focus on algorithms and systems for digital content creation. His current research topics include sketching, VR/AR/XR, and generative techniques, with applications in art, design, perception, and cultural heritage. His work has been recognized by an Adobe Research Fellowship, a Franke Interdisciplinary Research Fellowship, a Best Paper Award, and a Best Demo Honorable Mention Award.</p>
          </div>
        </div>
        <!-- Nick Bryan-Kinns -->
        <div class="col-lg-6 col-md-8 text-center">
          <div class="service-box mt-5 mx-auto">
            <a href="https://nickbknickbk.github.io/">
              <img class="mb-3" src="images/Nick-Bryan-Kinns.jpg" style="height: 200px; border-radius: 20px;" />
            </a>

            <h5 class="mb-3"><a target="_blank" rel="noopener noreferrer" href="https://nickbknickbk.github.io/"
                style="color: black;">Nick Bryan-Kinns</a></h5>

            <p class="text-muted mb-0" style="font-size: 0.9em;">University of the Arts London</p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">London, UK</p>
            <a href="mailto:n.bryankinns@arts.ac.uk" target="_blank">
              <p class="text-muted mb-0" style="font-size: 0.9em;">n.bryankinns@arts.ac.uk</p>
            </a>
            <br>

            <p class="text-muted mb-0" style="font-size: 0.9em; " align="left">Dr. Nick Bryan-Kinns is a Professor of Creative Computing at the Creative Computing Institute, University of the Arts London. His research explores new approaches to interactive technologies for the Arts and the Creative Industries through Creative Computing. His current focus is on Human-Centered AI and eXplainable AI for the Arts. His research has made audio engineering more accessible and inclusive, championed the design of sustainable and ethical IoT and wearables, and engaged rural and urban communities with physical computing through craft and cultural heritage. Products of his research have been exhibited internationally including Ars Electronica (Austria) the V&A and the Science Museum (UK), made available online and as smartphone apps, used by artists and musicians in performances and art installations, and have been reported in public media outlets including the BBC and New Scientist. He is a Fellow of the Royal Society of Arts, Fellow of the British Computer Society (BCS), and Senior Member of the Association of Computing Machinery (ACM). He is a recipient of the ACM and BCS Recognition of Service Awards, and chaired the ACM Creativity and Cognition conference 2009, and the BCS international HCI conference 2006.</p>
          </div>
        </div>
        <!-- Ambarish Natu -->
        <div class="col-lg-6 col-md-8 text-center">
          <div class="service-box mt-5 mx-auto">
            <a href="https://www.linkedin.com/in/ambarishnatu/">
              <img class="mb-3" src="images/Ambarish_Natu.jpg" style="height: 200px; border-radius: 20px;" />
            </a>

            <h5 class="mb-3"><a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/ambarishnatu/"
                style="color: black;">Ambarish Natu</a></h5>

            <p class="text-muted mb-0" style="font-size: 0.9em;">Australian Government</p>
            <p class="text-muted mb-0" style="font-size: 0.9em;">Australian Capital Territory, Australia</p>
            <a href="mailto:ambarish.natu@gmail.com" target="_blank">
              <p class="text-muted mb-0" style="font-size: 0.9em;">ambarish.natu@gmail.com</p>
            </a>
            <br>

            <p class="text-muted mb-0" style="font-size: 0.9em; " align="left">Dr. Ambarish Natu is with the Australian Government. After graduating from University of New South Wales, Sydney, Ambarish has held positions as a visiting researcher in Italy and Taiwan, worked for industry in United Kingdom and the United States of America and for the past ten years has been working in the Australian Government. For the past 17 years, Ambarish has led the development of five international standards under the auspices of the International Standards Organization (ISO) popularly known as JPEG (Joint Photographic Experts Group). He is the recipient of the ISO/IEC certificate for contributions to technology standards. Ambarish is highly active in the area of international standardization and voicing Australian concerns in the area of JPEG and MPEG (Motion Pictures Experts Group) standardization. He previously initiated an effort in the area of standardization relating to Privacy and Security in the Multimedia Context both within JPEG and MPEG standard bodies. In 2015, Ambarish was the recipient of the prestigious Neville Thiele Award and the Canberra Professional Engineer of the Year by Engineers Australia. Ambarish currently works as an ICT Specialist for the Australian Government. Ambarish is a Fellow of the Australian Computer Society and Engineers Australia. Ambarish also serves on the IVMSP TC and the Autonomous Systems Initiative of the IEEE Signal Processing Society. Ambarish has also been General Chair of DICTA 2018, ICME 2023 and TENSYMP 2023 in the past. Ambarish has keen interest in next generation data and analytics technologies that will change the course of the way we interact with in the world.</p>
          </div>
        </div>



      </div>
    </div>
  </section>

  <section class="bg-primary" id="history">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">History</h2>
          <hr class="light my-4">

          <h5 class="section-heading text-white" style="text-align: left; line-height: 1.8">
            <ul>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  AIART 2019: <a href="https://aiart2019.github.io/" target="_blank">https://aiart2019.github.io/</a>
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  AIART 2020: <a href="https://aiart2020.github.io/" target="_blank">https://aiart2020.github.io/</a>
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  AIART 2021: <a href="https://aiart2021.github.io/" target="_blank">https://aiart2021.github.io/</a>
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  AIART 2022: <a href="https://aiart2022.github.io/" target="_blank">https://aiart2022.github.io/</a>
                </div>
              </li>
              <li style="color: white;">
                <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
                  AIART 2023: <a href="https://aiart2023.github.io/" target="_blank">https://aiart2023.github.io/</a>
                </div>
              </li>
          </h5>
        </div>
      </div>
    </div>
  </section>

  <section id="MIR">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading">Partner: Machine Intelligence Research</h2>
          <hr class="dark my-4">
          <div class="text-muted" style="text-align: left;color: black;font-size: 18px;line-height: 1.8"> Machine
            Intelligence Research (original title: International Journal of Automation and Computing), published by
            Springer, and sponsored by Institute of Automation, Chinese Academy of Sciences, is formally released in
            2022. The journal publishes high-quality papers on original theoretical and experimental research, targets
            special issues on emerging topics and specific subjects, and strives to bridge the gap between theoretical
            research and practical applications. The journal has been indexed by ESCI, EI, Scopus, CSCD, etc.</div>
          <br>

          <ul>
            <li style="color: black;">
              <div class="text-muted" style="text-align: left;color: black;font-size: 18px;line-height: 1.8">
                MIR official websites:
                <ul>
                  <li>
                    <a style="text-align: left" href="https://www.springer.com/journal/11633"
                      target="_blank">https://www.springer.com/journal/11633</a>
                  </li>
                  <li>
                    <a style="text-align: left" href="https://www.mi-research.net"
                      target="_blank">https://www.mi-research.net</a>
                  </li>
                </ul>
              </div>
            </li>
            <li style="color: black;">
              <div class="text-muted" style="text-align: left;color: black;font-size: 18px;line-height: 1.8">
                MIR Editor-in-Chief - Tan Tieniu, Institute of Automation, Chinese Academy of Sciences
              </div>
            </li>
            <li style="color: black;">
              <div class="text-muted" style="text-align: left;color: black;font-size: 18px;line-height: 1.8">
                MIR Associate Editors-in-Chief
                <ul>
                  <li>
                    <a> Liang Wang, Chinese Academy of Sciences, China
                  </li>
                  <li>
                    <a> Yike Guo, Imperial College London, UK
                  </li>
                  <li>
                    <a> Brian C. Lovell, The University of Queensland, Australia
                  </li>
                </ul>
              </div>
            </li>

          </ul>

        </div>
      </div>
    </div>
  </section>

  <section class="bg-primary" id="sponsorship">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Sponsorship</h2>
          <hr class="light my-4">

          <h5 class="section-heading text-white" style="text-align: left; line-height: 1.8">

            <!-- <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">

              <strong>Platinum Level (RMB￥100,000)</strong><br>
              - 4 free registrations (or including up to 4 full registration)<br>
              - Invitation to give an industry keynote speech<br>
              - Logo on AIART 2024 official website with description and link to sponsor website<br>
              - Logo on workshop handbook and presentation material (under Platinum Level)<br>
              - One on one negotiation for special requirements.<br><br>

              <strong>Gold Level (RMB￥50,000)</strong><br>
              - 2 free registrations (or including up to 2 full registration)<br>
              - Participation in related industry panel<br>
              - Logo on AIART 2024 official website with short description and link to sponsor website<br>
              - Logo on workshop handbook and presentation material (under Gold Level)<br>
              - One on one negotiation for special requirements.<br><br>

              <strong>Silver Level (RMB￥20,000)</strong><br>
              - 1 free registration (or including up to 1 full registration)<br>
              - Logo on AIART 2024 official website with link to sponsor website<br>
              - Logo on workshop handbook and presentation material (under Silver Level)<br><br>

            </div> -->

          </h5>
          <br><br>
          <a>
            <img src="images/pku_art_logo.png" style="max-width: 90%; text-align:center; " />
          </a>
          <br><br><br>


          <h5 class="section-heading text-white" style="text-align: left; line-height: 1.8">

            <div class="text-faded" style="text-align: left;color: white;font-size: 18px;line-height: 1.8">
              Founded in 2024, the Computing Art Laboratory was established by the School of Art at Peking University and iFLYTEK,
              dedicated to exploring the infinite possibilities of the fusion of technology and art. The research of the Computing Art
              Laboratory covers a wide range of fields, including AI painting, art installations, digital human theater, VR games, VR
              short films, and so on. Characterized by the integration and co-construction of art, science and technology, the
              Computing Art Laboratory carries out in-depth interdisciplinary research and academic exchanges, and cultivates
              application-oriented talents for the society.<br><br>

              <!-- Tezign has raised the D1 round of financing and become the only Contech unicorn enterprise with a
              valuation of more than US $1 billion. Tezign is backed by world recognized investors including Temasek,
              Sequoia Capital, Hearst Ventures, among others. Tezign has partnered with more than 200 medium to large
              enterprises such as Alibaba, Unilever, ByteDance, PepsiCo, Shiseido, P&G, Starbucks, McDonald's, Heinz,
              Mars, Budweiser, Adidas, Xtep, Ubras, Lenovo, Midea, Tencent, L'Oreal, Danone, Porsche, Audi, Volvo,
              Aptar, Bosch, Stanley Black & Decker etc to upgrade their content strategies. It has produced more than
              150,000 creative content assets and formed a one hundred million level content asset management scale.
              Tezign's content ecosystem has gathered more than 50,000 content creators. <br><br>

              Tezign was a National Industrial Design Center issued by the Ministry of industry and information
              technology, awarded as TOP.1 in China in the list of The Information 50 startups, Fast Company top 50 of
              China's best innovative companies, Forbes High Growth Gazelle, Forbes China Top 10 intelligent design
              enterprises, Hurun Group List of Global Unicorn, and was named by Forrester, a leading independent global
              technology and market research company, in the Research Report "Now Tech: Marketing Resource Management,
              Q12022" (MRM) as the only MRM vendor in the contech segment Asia Pacific. <br><br>

              On 6th-7th May 2023, Tezign will launch the AIGC Builder and Creator Conference in Shanghai, to create
              maximum two-way interaction among AIGC creators and builders in various quality forms. Stay tuned by
              following the official Wechat account: Who is AIGC. <br> -->
            </div>

          </h5>
        </div>
      </div>
    </div>
  </section>


  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
  <script src="vendor/scrollreveal/scrollreveal.min.js"></script>
  <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/creative.min.js"></script>

</body>

</html>